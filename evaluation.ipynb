{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pickle\n",
        "import random\n",
        "import torch.nn as nn\n",
        "\n",
        "# Set seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Define the RNN model class\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size, 64)  # Adjust based on earlier needs\n",
        "        self.fc2 = nn.Linear(64, 32)            # Intermediate layer\n",
        "        self.fc3 = nn.Linear(32, num_classes)   # Output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)  # Initialize hidden state\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc1(out[:, -1, :])  # Only take the output from the last time step\n",
        "        out = self.fc2(out)             # Intermediate layer\n",
        "        out = self.fc3(out)             # Output layer\n",
        "        return out\n",
        "\n",
        "# Model parameters\n",
        "input_size = 14  # Ensure this matches your input feature size\n",
        "hidden_size = 64  # Hidden size during training\n",
        "num_layers = 2  # Number of RNN layers during training\n",
        "num_classes = 5  # Number of output classes (e.g., 5 plates)\n",
        "\n",
        "# Instantiate the model\n",
        "model = RNNModel(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, num_classes=num_classes)\n",
        "\n",
        "# Load state_dict with strict=False to avoid mismatch issues (if applicable)\n",
        "try:\n",
        "    model.load_state_dict(torch.load('/content/rnn_trained_model (1).pth'), strict=False)\n",
        "except RuntimeError as e:\n",
        "    print(f\"Error loading model state_dict: {e}\")\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Function to process input data\n",
        "def process_data(traj):\n",
        "    \"\"\"\n",
        "    Process the input trajectory data for the model.\n",
        "\n",
        "    Input:\n",
        "        traj: trajectory data containing longitude, latitude, time, and status.\n",
        "\n",
        "    Output:\n",
        "        data: processed data as a tensor suitable for model input.\n",
        "    \"\"\"\n",
        "    # Convert trajectory to DataFrame\n",
        "    df = pd.DataFrame(traj, columns=['longitude', 'latitude', 'time', 'status'])\n",
        "\n",
        "    # Convert 'time' to datetime and extract features\n",
        "    df['time'] = pd.to_datetime(df['time'])\n",
        "    time_features = ['month', 'day', 'dayofweek', 'hour', 'minute', 'second']\n",
        "    for feature in time_features:\n",
        "        df[feature] = getattr(df['time'].dt, feature)\n",
        "        df[f'{feature}_sin'] = np.sin(2 * np.pi * df[feature] / df[feature].max())\n",
        "        df[f'{feature}_cos'] = np.cos(2 * np.pi * df[feature] / df[feature].max())\n",
        "\n",
        "    # Drop original time column and status\n",
        "    df = df.drop(columns=['time', 'status'] + time_features)\n",
        "\n",
        "    # Normalize longitude and latitude\n",
        "    scaler = StandardScaler()\n",
        "    df[['longitude', 'latitude']] = scaler.fit_transform(df[['longitude', 'latitude']])\n",
        "\n",
        "    # Convert DataFrame to tensor\n",
        "    data = torch.FloatTensor(df.values).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    return data\n",
        "\n",
        "# Function to run the model and get predictions\n",
        "def run(data, model):\n",
        "    \"\"\"\n",
        "    Get the predicted label from the model for the processed data.\n",
        "\n",
        "    Input:\n",
        "        data: the output of the process_data function.\n",
        "        model: your trained model.\n",
        "\n",
        "    Output:\n",
        "        prediction: the predicted label (plate) of the data, an int value.\n",
        "    \"\"\"\n",
        "    with torch.no_grad():  # Disable gradient calculations for inference\n",
        "        output = model(data)\n",
        "\n",
        "        # Get the predicted label (plate) as an int value\n",
        "        prediction = output.argmax(dim=1).item()  # Assuming classification\n",
        "\n",
        "    return prediction\n",
        "\n",
        "# Load the test data from 'test.pkl'\n",
        "with open('/content/test.pkl', 'rb') as f:\n",
        "    test_data = pickle.load(f)\n",
        "\n",
        "# Evaluate model on test data\n",
        "results = []\n",
        "for trajectory in test_data:\n",
        "    processed_data = process_data(trajectory)\n",
        "    prediction = run(processed_data, model)\n",
        "    results.append(prediction)\n",
        "\n",
        "# Print results\n",
        "print(\"Predictions for test trajectories:\")\n",
        "for i, pred in enumerate(results):\n",
        "    print(f\"Trajectory {i+1}: Predicted Driver {pred}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WNC6Yge5Eu9M",
        "outputId": "d558a79e-4e22-4d04-b757-2ab2ec5ede3b"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions for test trajectories:\n",
            "Trajectory 1: Predicted Driver 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-42-526efbbc6ecf>:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('/content/rnn_trained_model (1).pth'), strict=False)\n"
          ]
        }
      ]
    }
  ]
}